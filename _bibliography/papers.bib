---
---

@string{aps = {American Physical Society,}}


@Article{arc_project_induction_transduction,
    abbr={arXiv},
    title={Combining Induction and Transduction for Abstract Reasoning}, 
    author={Wen-Ding Li* and Keya Hu* and Carter Larsen and Yuqing Wu and Simon Alford and Caleb Woo and Spencer M. Dunn and Hao Tang and Michelangelo Naim and Dat Nguyen and Wei-Long Zheng and Zenna Tavares and Yewen Pu＊ and Kevin Ellis＊},
    year={2024},
    month={November},
    pdf={2411.02272v4.pdf},
    archivePrefix={arXiv},
    arxiv={arXiv:2411.02272}, 
    preview={arc_show.jpg},
    selected={true}
}

@Article{tang2024coderepairllmsgives,
    abbr={NeurIPS},
    title={Code Repair with LLMs gives an Exploration-Exploitation Tradeoff}, 
    author={Hao Tang and Keya Hu and Jin Peng Zhou and Sicheng Zhong and Wei-Long Zheng and Xujie Si and Kevin Ellis},
    year={2024},
    month={July},
    journal={NeurIPS},
    eprint={2405.17503},
    archivePrefix={arXiv},
    primaryClass={cs.SE},
    pdf={2405.17503v2.pdf},
    arxiv={arXiv:2405.17503}, 
    preview={REx.jpg},
    selected={true}
}

@Article{embc_contrastive,
  abbr={EMBC},
  title={Contrastive Self-Supervised EEG Representation Learning for Emotion Classification},
  author={Keya Hu and Ren-Jie Dai and  Wen-Tao Chen and Hao-Long Yin and Bao-Liang Lu and Wei-Long Zheng},
  journal={EMBC},
  abstract={Self-supervised learning provides an effective approach to leverage a large amount of unlabeled data. Numerous previous studies have indicated that applying self-supervision to physiological signals can yield better representations of the signals. In the paper, we aim to apply this method to the crucial field of emotion recognition. We perform the experiment with several state-of-the-art contrastive self-supervised methods to explore their effectiveness in pre-training feature encoders on raw electroencephalography (EEG) signals and fine-tuning the pre-trained encoders on the downstream emotion classification tasks. We attempt to vary the proportion of labeled data used during fine-tuning and find that the improvement from self-supervised methods is more pronounced when the proportion of labeled data is small. Additionally, we explore the transferability of the feature encoders pre-trained on various datasets and observe that most self-supervised methods exhibit a certain degree of transferability. Methods that effectively utilize the temporal information in EEG signals show superior stability, accuracy, and transferability.},
  pdf={Contrastive Self-supervised EEG Representation Learning for Emotion Classification.pdf},
  year={2024},
  month={July},
  preview={embc2.jpg},
  slides={ssl_eeg_pre.pdf},
  selected={true}
}

